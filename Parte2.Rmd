---
title: ""
author: "Gabriel Perez - gabrielmpp2@gmail.com"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---


# Parte 2 - Instalação de pacotes

Além dos recursos fornecidos no r-base, o R conta com mais de 10 mil pacotes disponíveis no CRAN e mais milhares disponíveis no Github. Os pacotes são essenciais para uma boa experiência com o R, pois podem:  
  

* facilitar a forma de programar;
  
    + dplyr (%>%)

* acelerar o tempo de processamento;  
  
    + gpuR   

    + Parallel
  
* melhorar os recursos gráficos;  
  
    + ggplot2
    + raster e rasterVis
    + fields
    + lattice e latticeExtra
  
* gerar modelos / resolver equações em uma linha de comando;  
  
    + caret
    + h2o
    + odesolve (equações diferenciais)
  
* diagramar notebooks como esse;  
  
    + RMarkdown
  
* integrar o R com outras linguagens;  
  
    + RCpp
    + RFortran
    + rJava
  
  
> Vamos visitar o CRAN: https://cran.r-project.org/  
  
## Instalando pacotes do CRAN  
  
Os pacotes disponíveis no CRAN são os mais fáceis de instalar, o comando para isso é:
  
```{r,eval=FALSE}
install.packages(pkgs="nome do pacote",lib="diretorio")

# e para remover temos o comando equivalente:
remove.packages(pkgs = "nome do pacote",lib="diretorio")
```
  
  
Tentem instalar o pacote "devtools".  
  
## Instalando pacotes via Github
  
O pacote devtools possibilita instalar pacotes diretamente do github. no github podemos encontrar pacotes que ainda não se encontram no CRAN ou baixar versões mais recentes e/ou em desenvolvimento.
  

```{r,eval=FALSE}
library(devtools)
install_github("nome do repositorio")

```
  
  
Em ocasiões muito específicas pode ser necessário compilar manualmente o código-fonte de algum pacote. Não vamos abordar essa situação pois cada pacote tem sua particularidade. Nesse caso o mais adequado é buscar um passo-a-passo na internet.

  

# Análise de exploratória em dados de estações do INMET
  
Para começarmos a fazer algumas análises estatísticas no R, vamos utilizar dados de estações automáticas do INMET. Os dados podem ser baixados no link abaixo:

> https://drive.google.com/open?id=1LLE61RbQ1EhFpwSSZzTsAIN0kbVSZwxw

Agora vamos criar um projeto no RStudio chamado "AnaliseINMET". Uma vez criado o projeto, vamos mover os dados baixados para um diretório chamado "dados" dentro do diretório do projeto. Dessa forma o acesso fica mais simples. Abra um novo script no projeto e salve com algum nome como "analise.R". Escolha uma das estações diponíveis e abra o arquivo txt no __Gedit__.  
 
__Tarefa__  
Procure a documentação da função `read.table` com a função `help("read.table")` e utilize os argumentos da para fazer a leitura da tabela considerando somente as linhas de interesse, ou seja, a partir da linha que contém os nomes das variáveis. Desejamos também manter os nomes da colunas com os nomes das variáveis.

```{r, echo = FALSE}
df_sp <- read.table("dados/inmet_saopaulo.txt",skip = 16, header = TRUE, sep = ";", row.names = NULL)

# se nossa tabela fosse um arquivo Excel, basta usar o comando read.csv

## digite somente "df_sp" no lugar do comando abaixo
kable(head(df_sp))

```
  
Surgiu uma coluna nomeada "X" com valores ausentes. Isso aconteceu pois o arquivo do INMET possui um último " ; " depois da última coluna de dados, e o R entende que depois dele virá uma nova coluna. Vamos removê-la:  
  
```{r}
df_sp$X <- NULL

## Vamos remover o número da estação que não será útil

df_sp$Estacao <- NULL
summary(df_sp)
```
  
Vamos fazer a média diária das variáveis. Para isso, vamos usar precisar manipular a coluna de datas.  É conveniente neste momento usar o comando `sapply`. Esse comando faz parte de uma família de comandos cujo o objetivo é aplicar uma função a um data.frame, lista ou vetor.  
  
> lapply e sapply  
  
Aplicam uma função a uma lista ou vetor. O `lapply` retorna uma lista e o `sapply` retorna um vetor.  
  
> apply  
  
Aplica uma função nas colunas ou linhas de um data.frame  
  
> mcapply
  
Vale ainda mencionar o `mcapply` da biblioteca parallel. Esse comando possui a mesma funcionalidade do apply e possibilita controlar quantos núcleos do processador serão utilizados
  

```{r}
# vamos criar colunas separadas para dia, mês e ano usando o strsplit e o sapply

# primeiro testem o comando strsplit e vejam a saída, removam o # da linha abaixo
# strsplit(x = as.character(df_sp$Data),split = "/")

df_sp$mes <- sapply(strsplit(x = as.character(df_sp$Data),split = "/"),FUN = '[',2)
df_sp$ano <- sapply(strsplit(x = as.character(df_sp$Data),split = "/"),FUN = '[',3)

# podemos escrever  o mesmo comando usando a sintaxe com %>%, muitos usuários
# argumentam que essa sintaxe possui mais clareza

library(dplyr)
df_sp$mes <- df_sp$Data %>% as.character() %>% strsplit(split="/") %>% sapply(FUN='[',2)
df_sp$ano <- df_sp$Data %>% as.character() %>% strsplit(split="/") %>% sapply(FUN='[',3)

```
  
  
Agora temos a data separada em colunas, podemos fazer médias diárias, mensais e anuais. Para isso podemos usar o comando `aggregate`.  A função aggregate aceita objetos do tipo __fórmula__ como padrão de agregamento. Veja `help(stats::aggregate)`.
  
```{r}
temp_mensal <- aggregate(TempBulboSeco ~ mes + ano, data = df_sp, mean)
kable(temp_mensal) #digite somente temp_mensal 
plot(temp_mensal)
plot(temp_mensal$TempBulboSeco,type="l")
```

Agora vamos colocar as datas no eixo X e arrumar os nomes dos eixos.  
  
```{r}
datas <- seq(as.Date("1998-01-01"),as.Date("2017-12-31"),by="month")
plot(x=datas, y=temp_mensal$TempBulboSeco, xlab = "Data", ylab = "Temperatura ºC",type = "l")
```

___Exercício___ 
  
Faça o mesmo processo para obter um vetor com as médias diárias e um vetor com a temperatura média de cada mês (12 valores). Faça os plots correspondentes com as datas no eixo x.  
  
## Transformada de Fourier
  
Os plots preliminares revelam que existem oscilações periódicas nas variáveis observadas. É natural tentar entender quais são as frequências dominantes nessas séries temporais.  
  
Vamos explorar a função nativa do r `spec.pgram`:
```{r}
help("spec.pgram")

```
  
  
Notem que a função `spec.pgram` espera como entrada uma variável do tipo "time series". Vamos transformar nosso vetor de observações em uma variável `ts`.  
  

```{r}
ts_temp_sp <- ts(temp_mensal$TempBulboSeco,start = 1998, end = 2018, frequency = 12) 
class(ts_temp_sp)
plot(ts_temp_sp) # Note que ao plotar uma variável ts o eixo x já recebe corretamente as datas
```

__Exercício__
É uma boa prática normalizar a série temporal antes de fazer a análise de Fourier, i.e. $\bar{x}=0 $ e $x^2 \le1$. Normalize o vetor `ts_temp_sp`.
```{r,echo=FALSE}
# É uma boa prática normalizar a ts antes de fazer a transformada de foutier
ts_temp_sp <- (ts_temp_sp - mean(ts_temp_sp)) / max(ts_temp_sp)

```
  
Depois de normalizado, vamos fazer a transformada no vetor.  
```{r}
fft_temp <- spec.pgram(ts_temp_sp, plot=FALSE)

# Vamos examinar o objeto fft_temp
fft_temp

# O objeto é uma lista onde "spec" contém a potência de cada frequência "freq"
# Para obter o período basta inverter a frequência

fft_temp$period <- 1/fft_temp$freq

plot(x = fft_temp$period, y = fft_temp$spec,type = "l")

# Por último, vamos criar um data.frame com as informações relevantes da transformada

df_fft_temp <- data.frame(period = fft_temp$period, spec = fft_temp$spec)

```
  
  
Naturalmente, o plot mostra que o período anual é o mais importante para a temperatura. No entanto, vamos gerar uma tabela com os períodos mais importantes em ordem decrescente.  
  
> Tarefa: Vamos procurar no Google como ordenar uma tabela no R usando os valores de uma coluna. Queremos ordenar nossa tabela fft_temp com a coluna spec

```{r,echo=FALSE,eval=FALSE}
"how to sort a dataframe by columns in R - stackoverflow"
```

> O que os períodos mais energéticos podem nos dizer?

```{r,echo=FALSE}
library(dplyr)


kable(head(arrange(df_fft_temp, desc(spec)),10))

```


## Matriz de correlação
  
A matriz de correlação é uma ferramenta útil na análise exploratória de uma base de dados e quantifica a relação linear entre todas as variáveis da tabela. No R, o comando é o seguinte:  
  
```{r, eval = FALSE}
cor(x, method = c("pearson","kendall","spearman")) 
# o método default é o pearson, mas podemos escolher entre os três
```
  
A correlação de Pearson é dada pela seguinte equação:  
$$ r_{xy} = \frac{Cov(X,Y)}{S_xS_y} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum(x_i-\bar{x})^2(y_i-\bar{y})^2}}$$
Onde $S$ é o desvio padrão.  
  
Agora vamos analisar nosso data.frame `df_sp`. Devemos inicialmente remover as colunas com valores não numéricos.  
```{r}
names(df_sp)
df_sp_num <- df_sp[,c(-1,-2,-10,-11)]
cor(df_sp_num)
```

Todas as correlações resultaram como `NA` pois existem valores faltantes na tabela. Uma possível solução para isso é usar a função `na.omit`.  
```{r}
m_cor <- df_sp_num %>% na.omit() %>% cor()
# m_cor <- cor(na.omit(df_sp_num))

kable(m_cor,digits = 2)
```
  
```{r}
library(corrplot)
corrplot.mixed(m_cor, lower.col = "black", number.cex = .7)

```
__Para discutir__  
   
   Discutam com o colega de mesa as seguintes questões.
   
    - Por que a temperatura e a direção do vento se correlacionam positivamente?  
    
    - Por que a temperatura e a nebulosidade se correlacionam negativamente?  
    
    - Por que a temperatura e a umidade relativa se correlacionam negativamente?  
      
    

## Regressão linear 

Modelagem estatística talvez seja o ponto mais forte do R. O pacote nativo `stats` conta com uma ampla coleção de técnicas e modelos. Entretanto, existem pacotes que expandem as capacidades do R e possibilitam a utilização de técnicas modernas e altamente eficientes do ponto de vista computacional. Quem deseja trabalhar com modelos mais complexos vale a pena investigar pacotes como keras, caret, h2o e mxnet.  
  
Vamos começar trabalhando com os comandos nativos.  
  
```{r}
names(df_sp)

df_lm <- df_sp[ , c(3,4,5,6,7,8,9)]

### Agora vamos normalizar nosso dataframe usando apply

normalize <- function (x) {
  x/max(x,na.rm = TRUE)
}
```


__Exercício__
Usando o comando `apply`, aplique a função `normalize` que acabamos de criar nas colunas do dataframe `df_lm`. Verifique se a saída desse comando será da classe `data.frame`, caso não seja, transforme nessa classe.  
  
  
```{r, echo = FALSE}
df_lm <- as.data.frame(apply(df_lm, 2, normalize)) # estamos aplicando o as.data.frame pois o apply retorna uma matriz

```

Finalmente, vamos aplicar o modelo linear:
```{r}
modLinear <- lm(TempBulboSeco ~ PressaoAtmEstacao, data = df_lm)

```
  

> Agora vamos interpretar a regressão  
  

```{r}

summary(modLinear)

```
  

Volte na tabela de correlação e compare o coeficiente de pearson com o R² da regressão.  
  
É possível plotar a reta da regressão linear:  
  
```{r}
plot(x = df_lm$PressaoAtmEstacao, y = df_lm$TempBulboSeco) 
abline(a = modLinear$coefficients[1], b = modLinear$coefficients[2], col = "red")
```


  
__Atividade__
  

Aplique a regressão linear múltipla considerando todas as variáveis do `df_lm` e interprete a saída da função `summary`. Dica: é necessário adaptar a fórmula da função `lm()`. Se não encontrar ajuda na documentação da função, busque no Google.  

---

Se quisermos prever uma variável categórica (não-contínua) a regressão linear não é o modelo mais adequado. Existem outros modelos mais adequados como a regressão logística 
  
## Regressão logística  
  
Como vimos no plot anterior, a variável nebulosidade não é contínua e uma regressão linear apresenta uma saída contínua, portanto não é o método mais adequado. A regressão logística, no entanto, funciona como um classificador linear e é mais adequado à variáveis categóricas. A função logística pode ser dada por:  
  
$$f(y)=\frac{1}{1+e^{-y}} $$

A regressão logística consiste em aplicar a função logística em um modelo linear $y=ax+b$. 
  
$$f(ax+b)=\frac{1}{1+e^{-ax-b}} $$
As constantes a e b podem ser encontradas invertendo a função logística e aplicando o método dos mínimos quadrados.  
  

A saída da função logística pode ser interpretada como a probabilidade de um evento ocorrer. Embora a função seja não-linear, o classificador é linear pois é definido um threshold (limiar) de decisão, em geral esse limiar é 0.5. Veja o gráfico da função logística:  
  
```{r,echo=FALSE}
logistic <- function (x) {1/(1+exp(-x))}
x <- seq(-10,10,0.01)
y <- logistic(x)
temp<-data.frame(x,y)
theme_set(theme_bw())

 ggplot(temp, aes(x = x, y=y)) + 
  geom_line() +
  theme(axis.line = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  labs(x="X", fill=NULL,
       y="Y", 
       title="Função Logística")
```
  
A regressão logística pode ser computada pelo comando glm do pacote stats (nativo). Essa função é uma extensão mais flexível do lm.  

```{r}
df_lm$Nebulosidade <- as.factor(df_lm$Nebulosidade)
logisticReg <- glm(Nebulosidade ~  . , data = df_lm, family = binomial(link="logit"))
```
  
## Análise de componentes principais (PCA)
  
A PCA é uma transformação linear ortogonal (W) que projeta os dados (X) em uma base onde as variáveis não possuem correlação linear (Z).  As colunas de W são os autovetores da matriz de covariância de X.
$$Z_{n,m} = X_{n,m}W{m,m}$$
A PCA é útil para (1) reduzir a dimensão da base de dados (2) encontrar os padrões lineares dominantes na série.  
  
Vamos então aplicar a PCA com a função `prcomp` no nosso dataframe `df_sp`.  Como o ciclo diurno vai causar um sinal muito forte, vamos pegar somente o horário das 18Z.
  
```{r}
# Vamos informar para o R que a coluna de "Data" do df_sp realmente se trata de variáveis tipo date
df_sp$Data<-as.Date(df_sp$Data,format="%d/%m/%Y")


df_pca <- df_sp[which(df_sp$Hora=="1800"),c(1,3,4,5,6,7,8,9)] 

# Pegar somente as linhas sem observações faltantes (NA)

mask <- complete.cases(df_pca)
df_pca <- df_pca[mask,]

# Vamos transformar as colunas do df_pca em variáveis numéricas

df_pca_numeric <- apply(df_pca[,-1],2,as.numeric)

pca <- prcomp(df_pca_numeric, center = TRUE, scale = TRUE)
summary(pca)

```
  
Observe que com 5 componentes já é possível explicar mais de 90% da variância da série.  
  
```{r}
plot(pca, type="l")
```

Para prever as componentes principais de novas observações ou das observações que usamos para calcular a PCA podemos utilizar a função predict:  
    
```{r}
PCA <- predict(pca, df_pca)
# precisamos remover os dias ocorreram dados ausentes do vetor de datas 
plot(PCA[,1], x = df_pca$Data, type = "l", xlab = "Data", ylab = "PCA1")
```
O ciclo anual aparece de maneira evidente na primeira componente principal.  
  
Um biplot é útil para extrair informações das componentes principais. Para isso vamos usar o pacote gráfico `ggplot2` e o pacote auxiliar `ggbiplot` que deve ser instalado do github:  
  
```{r,eval=FALSE}
library(devtools)
install_github("vqv/ggbiplot")
```

```{r}
library(ggplot2)
library(ggbiplot)
mes <- format(df_pca$Data,"%m")
  
g <- ggbiplot(pca, obs.scale = 1, var.scale = 3,var.axes = TRUE,
              groups = mes,alpha = 0.3, 
              ellipse = TRUE) 
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
               legend.position = 'top')
print(g)
```
  
  
__Salvando o output gráfico__  
  
Para salvar os gráficos do R, existem as funções png(), gif(), jpeg(), tiff, pdf(), entre outras. Procure na documentação do R sobre a função png() e exporte o gráfico que fizemos acima.  
  

# Aprimorando os gráficos com o ggplot2
  
O `ggplot2` é um pacote tão grande do R que quase poderia ser considerado outra linguagem. Vamos seguir os tutoriais abaixo. 
  

http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html   
  

http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html
  
  
# Exercício  
  
1 - Reproduza os plots da parte 2 usando o ggplot2 e pense em um novo plot baseado nas ideias do top50. Salve a saída dos plots que fizer e envie para `gabrielmpp2@gmail.com`.  
  

2 - Explore as formas de plotar a matriz de correlação sugeridas no manual abaixo. Plote a matriz de correlação que exploramos hoje e envie para o mesmo e-mail.  
  
3 - Instale o pacote openair e use a função windrose para fazer um plot tipo rosa dos ventos somente com os dados da estação meteorológica escolhida nos meses de Dezembro, Janeiro e Fevereiro


